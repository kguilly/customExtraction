/*
This file will test if its possible to send a file pointer to CUDA and then extract the GRIB 2 file using 
an object generated by ECCODES that will indicate where to find the values 


*/



#include <iostream>
#include <stdlib.h>
#include "eccodes.h"
#include <cuda_runtime.h>
#include <vector>
#include <string>

// personal functions
#include "decompress_grib_test.h"
#include "shared_test_objs.h"

station_t* extract_indexes(station_t * local_st_arr, double* lats, double* lons, int numStations, int numberOfPoints){

    int num_threads_to_use = 256;
    int num_blocks_to_use = 2;

    // make device copies of host params
    station_t* d_stationArr_findNearest;
    double* d_lats;
    double* d_lons;
    

    // allocate each of the arguments to the GPU
    if (cudaMalloc(&d_stationArr_findNearest, sizeof(station_t) * numStations) != cudaSuccess) {
        std::cout << "stationArr could not be allocated to GPU" << std::endl;
        return local_st_arr;
    }
    if (cudaMalloc(&d_lats, sizeof(double) * numberOfPoints) != cudaSuccess) {
        std::cout << "Grib lats could not be allocated to the GPU" << std::endl;
        cudaFree(d_stationArr_findNearest);
        return local_st_arr; 
    }
    if (cudaMalloc(&d_lons, sizeof(double) * numberOfPoints) != cudaSuccess) {
        std::cout << "Grib lons could not be allocated to the GPU" << std::endl;
        cudaFree(d_stationArr_findNearest);
        cudaFree(d_lats);
        return local_st_arr;
    }

    // copy each of the arguments over to the gpu
    if (cudaMemcpy(d_stationArr_findNearest, local_st_arr, sizeof(station_t) * numStations, cudaMemcpyHostToDevice) != cudaSuccess) {
        std::cout << "The stationArr could not be copied to the GPU" << std::endl;
        cudaFree(d_stationArr_findNearest);
        cudaFree(d_lats);
        cudaFree(d_lons);
        return local_st_arr;
    }
    if (cudaMemcpy(d_lats, lats, sizeof(double) * numberOfPoints, cudaMemcpyHostToDevice) != cudaSuccess) {
        std::cout << "The grib lats could not be copied to the GPU" << std::endl;
        cudaFree(d_stationArr_findNearest);
        cudaFree(d_lats);
        cudaFree(d_lons);
        return local_st_arr;
    }
    if (cudaMemcpy(d_lons, lons, sizeof(double) * numberOfPoints, cudaMemcpyHostToDevice) != cudaSuccess) {
        std::cout << "The grib lons could not be copied to the GPU" << std::endl;
        cudaFree(d_stationArr_findNearest);
        cudaFree(d_lats);
        cudaFree(d_lons);
        return local_st_arr;
    }
    
    // call the kernel
    cuda_find_nearest <<< num_blocks_to_use, num_threads_to_use >>> (d_stationArr_findNearest, d_lats, d_lons, numStations, numberOfPoints);
    // wait for em all to finish
    cudaDeviceSynchronize();

    // copy the elements from the GPU back over to the host
    if (cudaMemcpy(local_st_arr, d_stationArr_findNearest, sizeof(station_t) * numStations, cudaMemcpyDeviceToHost) != cudaSuccess) {
        std::cout << "cuda FAILED" << std::endl;
        cudaFree(d_stationArr_findNearest);
        cudaFree(d_lats);
        cudaFree(d_lons);
        return local_st_arr;
    }
    std::cout << "cuda success" << std::endl;
    // release
    cudaFree(d_stationArr_findNearest);
    cudaFree(d_lats);
    cudaFree(d_lons);

    return local_st_arr;
}

__global__ void cuda_find_nearest(station_t * d_stationArr, double * d_lats, double * d_lons, int numStations, int num_points) {
    int id = blockIdx.x * blockDim.x + threadIdx.x;

    if (id < numStations) {
        double min_distance = 999;
        int min_index = -1;
        
        station_t * curr_station = &d_stationArr[id];
        double st_lat = curr_station->lat;
        double st_lon = curr_station->lon;

        for (int i=0; i<num_points; i++) {
            double lat = d_lats[i];
            double lon = d_lons[i]; 
            double distance = sqrt(pow((st_lat - lat), 2) + pow((st_lon - lon), 2));

            if (distance < min_distance) {
                min_distance = distance;
                min_index = i;
            }
        }
        curr_station->closestPoint = min_index;
    }
}


station_t* cuda_orchestrate_decompress_grib(station_t* loc_st_arr, char* full_path, const char*** passed_params, int numParams, int numStations) {
    
    // determine the number of blocks and the number of threads per block
    int num_blocks = 8;
    int num_threads = 256;

    // get the size of the file obj, may need for later
    // fseek(f, 0, SEEK_END);
    // long file_size = ftell(f);
    // fseek(f, 0, SEEK_SET);
    
    // make device copies of the local vars to pass
    station_t* dev_st_arr;
    char* d_full_path;
    int path_len = strlen(full_path);
    const char*** dev_passed_params;

    size_t passed_param_size = numParams * 2 * sizeof(const char*);

    // allocate enough space for the lats, lons, and values arrays
    if (cudaMalloc(&dev_st_arr, sizeof(station_t) * numStations) != cudaSuccess) {
        printf("Station array of %d station could not be copied to GPU\n", numStations);
        return loc_st_arr;
    }
    if (cudaMalloc(&d_full_path, path_len * sizeof(char)) != cudaSuccess) {
        // err, free space, return
        printf("File of size %d could not be copied over to GPU\n", path_len);
        cudaFree(dev_st_arr);
        return loc_st_arr;
    }
    if (cudaMalloc(&dev_passed_params, passed_param_size) != cudaSuccess) {
        // err, free space, return 
        printf("Param arr of size %d could not be copied over to GPU\n", passed_param_size);
        cudaFree(dev_st_arr);
        cudaFree(d_full_path);
        return loc_st_arr;
    }

    // copy over all that good stuff to the GPU
    if (cudaMemcpy(dev_st_arr, loc_st_arr, sizeof(station_t) * numStations, cudaMemcpyHostToDevice) != cudaSuccess) {
        printf("Could not copy over dev_st_arr\n");
        cudaFree(dev_st_arr);
        cudaFree(d_full_path);
        cudaFree(dev_passed_params);
        return loc_st_arr;
    }
    if (cudaMemcpy(d_full_path, full_path, path_len * sizeof(char), cudaMemcpyHostToDevice) != cudaSuccess) {
        printf("Could not copy over file\n");
        cudaFree(dev_st_arr);
        cudaFree(d_full_path);
        cudaFree(dev_passed_params);
        return loc_st_arr;
    }
    if (cudaMemcpy(dev_passed_params, passed_params, passed_param_size, cudaMemcpyHostToDevice) != cudaSuccess) {
        printf("Could not copy over passed_params\n");
        cudaFree(dev_st_arr);
        cudaFree(d_full_path);
        cudaFree(dev_passed_params);
        return loc_st_arr;
    }

    // call the kernel
    cuda_decompress_grib <<< num_blocks, num_threads >>> (dev_st_arr, d_full_path, dev_passed_params, numStations, numParams);
    cudaDeviceSynchronize();

    // copy the dev st arr back to the local
    if (cudaMemcpy(loc_st_arr, dev_st_arr, sizeof(station_t)*numStations, cudaMemcpyDeviceToHost) != cudaSuccess) {
        printf("CUDA failed\n");
        cudaFree(dev_st_arr);
        cudaFree(d_full_path);
        cudaFree(dev_passed_params);    
        return loc_st_arr;
    }
    else {
        printf("CUDA success\n");
    }

    // free all the stuff
    cudaFree(dev_st_arr);
    cudaFree(d_full_path);
    cudaFree(dev_passed_params);
    // allocate enough space for the passed params, and maybe the handle obj, and the index obj

}

__global__ void cuda_decompress_grib(station_t* st_arr, const char* full_path, const char *** passed_params, int numStations, int numParams) {
    // this function will thread the decompression on each of the passed params
    int id = blockIdx.x * blockDim.x + threadIdx.x;

    if (id < numParams) {
        const char * curr_param_name = passed_params[id][0];
        long curr_level = std::stol(passed_params[id][1]);
        
    }
}



